{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìó Prompt template Basics üìó ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3\",temperature=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÑ Prompt templates using from_template method üìÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tell me a funny joke about elephant\n",
      "text='tell me a funny joke about elephant'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 1st way of creating a prompt template. \n",
    "# using \"from_template\" method of PromptTemplate class.\n",
    "pt = PromptTemplate.from_template('tell me a funny joke about {content}')     # --> returns an object of PromptTemplate class.\n",
    "\n",
    "\n",
    "# we can format it just like string formatting.\n",
    "print(pt.format(content = 'elephant'))                  # --> prints prompt as string.\n",
    "\n",
    "\n",
    "# we can also invoke a pt object as it is runnable.\n",
    "final_prompt = pt.invoke({\"content\":\"elephant\"})        \n",
    "final_prompt                                            # --> returns StringPromptValue instance\n",
    "print(final_prompt)                                     # --> prints a string variable with prompt as its value.                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è prompt templates using constructors üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a good joke about elephant\n",
      "text='Tell me a funny joke about elephant'\n"
     ]
    }
   ],
   "source": [
    "# using PromptTemplate class, using PromptTemplate class constructor directly.\n",
    "\n",
    "pt = PromptTemplate(\n",
    "    template='Tell me a {adjective} joke about {content}',\n",
    "    input_variables=['adjective', 'content']\n",
    ")\n",
    "\n",
    "print(pt.format(adjective='good',content= 'elephant'))             # --> prints prompt as string.\n",
    "\n",
    "final_prompt = pt.invoke({\"adjective\" : \"funny\", \"content\" : \"elephant\"})\n",
    "final_prompt                                                        # --> returns StringPromptValue instance\n",
    "print(final_prompt)                                                 # --> prints a string variable with prompt as its value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü¶ô Send prompt to LLM ü¶ô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's one:\\n\\nWhy did the elephant quit his job at the circus?\\n\\nBecause he was tired of working for peanuts and wanted to trumpet his own success!\\n\\nHope that made you trumpet with laughter!\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# > we can send the prompt to llm by two ways. <#\n",
    "# >   1. Directly invoking the llm\n",
    "# >   2. Creating a chain of two objects.\n",
    "\n",
    "\n",
    "# 1st is directly invoking the llm object.\n",
    "# llm.invoke(final_prompt)                            # --> returns a non parsed string output.\n",
    "\n",
    "\n",
    "# 2nd way we can chain prompt and llm object. chain direction is left --> right.\n",
    "chain = pt | llm\n",
    "chain.invoke(\n",
    "    {\"adjective\": \"new\", \"content\": \"elephant\"}\n",
    ")  # --> returns a non parsed string output.\n",
    "\n",
    "# Second way with chaining is preferred !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìñ few shot example prompt templates üìñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=' A User is trying to teach you something new by giving examples below in the form of question and answers. For every question there is an answer provided, try to relate the answer to its respective question. learn the pattern through the question and answer pair given as example. While learning this pattern keep an open mind and try to answer the final question asked by the user based on this new pattern. Examples are - \\n\\n\\n    Question : what is 2 * 2 ?\\n    Answer : 4\\n\\n\\n\\n    Question : what is 5 * 3 ?\\n    Answer : 8\\n\\n\\n\\n    Question : what is 12 * 2 ?\\n    Answer : 14\\n\\n\\nQuestion: In the given examples please explain what does c stands for and what is 10 c 2 ?'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "# first create a list of dictionaries as examples.\n",
    "user_provided_examples: List[Dict[str, str]] = [\n",
    "    {\n",
    "        \"question\": \"what is 2 * 2 ?\",\n",
    "        \"answer\": \"4\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is 5 * 3 ?\",\n",
    "        \"answer\": \"8\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is 12 * 2 ?\",\n",
    "        \"answer\": \"14\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# then we will create a template showing how to present examples to the LLM.\n",
    "example_template = \"\"\"\n",
    "    Question : {question}\n",
    "    Answer : {answer}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# we will use that template to create a prompt_template\n",
    "example_pt = PromptTemplate(\n",
    "    template=example_template,\n",
    "    input_variables=[\"question\", \"answer\"],\n",
    ")\n",
    "\n",
    "# Checking the format of our examples prompt template.\n",
    "example_pt.invoke(user_provided_examples[0]).to_string()\n",
    "\n",
    "\n",
    "system_message = ''' A User is trying to teach you something new by giving examples below in the form of question and answers. For every question there is an answer provided, try to relate the answer to its respective question. learn the pattern through the question and answer pair given as example. While learning this pattern keep an open mind and try to answer the final question asked by the user based on this new pattern. Examples are - '''\n",
    "\n",
    "\n",
    "\n",
    "# Then we'll create a few shots prompt template using our examples and example_prompt_template\n",
    "fewshot_pt = FewShotPromptTemplate(\n",
    "    examples = user_provided_examples,\n",
    "    example_prompt = example_pt,\n",
    "\n",
    "    prefix = system_message,                # We can also pass a system message to using prefix.\n",
    "    suffix=\"Question: {question}\",          # suffix is a prompt_template for final question, which is not an example.\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Finally we'll invoke the fewshot_pt to create final prompt that will be sent to the llm.\n",
    "final_prompt = fewshot_pt.invoke({\"question\": \"In the given examples please explain what does c stands for and what is 10 c 2 ?\"})\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚õìÔ∏è‚Äçüí• Chaining all the parts together ‚õìÔ∏è‚Äçüí•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# use String parser to get output in string \n",
    "str_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "chain = fewshot_pt | llm | str_parser\n",
    "# chain.invoke({\"question\": \"what is 10 c 2 ?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü¶ô Stream the FewShotExample prompt result from LLM ü¶ô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the examples, I notice that the answers seem to be increasing by a fixed amount each time.\n",
      " Specifically:\n",
      "\n",
      "* 2 * 2 = 4 (increase of 2)\n",
      "* 5 * 3 = 8 (increase of 3)\n",
      "* 12 * 2 = 14 (increase of 2)\n",
      "\n",
      "This pattern suggests that '*' might mean \"add\" or \"increment by\", rather than the typical meaning of multiplication.\n",
      "\n",
      "If this is correct, then:\n",
      "\n",
      "* 12 * 12 would be an increase of 12 to 12, which would result in an answer of... 24!\n",
      "\n",
      "Am I on the right track?\n"
     ]
    }
   ],
   "source": [
    "line = ''\n",
    "\n",
    "# If we want to stream output line by line.\n",
    "for chunk in chain.stream({\"question\": \"Based on the examples what do you think '*' means and what is 12 * 12 ?\"}):\n",
    "    if chunk == '.' or chunk == '\\n':\n",
    "        line += chunk\n",
    "        print(line)\n",
    "        line = \"\"\n",
    "    \n",
    "    else:\n",
    "        line += chunk\n",
    "\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü™¢ Ollama Embeddings ü™¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n",
      "[-2.3860483169555664, 3.483680486679077, -3.3421359062194824, -1.697510004043579, 1.383670687675476]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embedder = OllamaEmbeddings(model=\"llama3\")\n",
    "\n",
    "embeddings  = embedder.embed_query('Hi My name is Sahil')\n",
    "\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìñSemantic similarity example selection üìñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples most similar to the input: what is 10 m 2 ?\n",
      "\n",
      "\n",
      "answer: 2.5\n",
      "question: what is 5 m 2 ?\n",
      "\n",
      "\n",
      "answer: 2.5\n",
      "question: what is 5 m 2 ?\n",
      "\n",
      "\n",
      "answer: 2.5\n",
      "question: what is 5 m 2 ?\n",
      "\n",
      "\n",
      "answer: 4\n",
      "question: what is 12 m 3 ?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#? from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "from typing import List, Dict\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Create an embedder object.\n",
    "embedder = OllamaEmbeddings(model=\"llama3\")\n",
    "\n",
    "\n",
    "# # then we will create a template showing how to present examples to the LLM.\n",
    "example_template : str = \"\"\"\n",
    "    Question : {question}\n",
    "    Answer : {answer}\n",
    "\"\"\"\n",
    "\n",
    "# first create an extensive list of dictionaries as examples.\n",
    "user_provided_examples: List[Dict[str, str]] = [\n",
    "    {\n",
    "        \"question\": \"what is 2 c 2 ?\",\n",
    "        \"answer\": \"4\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is 5 c 3 ?\",\n",
    "        \"answer\": \"15\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is 12 c 2 ?\",\n",
    "        \"answer\": \"24\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is 5 m 2 ?\",\n",
    "        \"answer\": \"2.5\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is 12 m 3 ?\",\n",
    "        \"answer\": \"4\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is 15 m 15 ?\",\n",
    "        \"answer\": \"1\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# Create an example selector \n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    user_provided_examples,                 # This is the list of examples available to select from.\n",
    "    embedder,                               # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    Chroma,                                 # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    k=4,                                    # This is the number of examples to produce.\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Select the most similar example to the input.\n",
    "question = \"what is 10 m 2 ?\"\n",
    "selected_examples = example_selector.select_examples({\"question\": question})\n",
    "print(f\"Examples most similar to the input: {question}\")\n",
    "for example in selected_examples:\n",
    "    print(\"\\n\")\n",
    "    for k, v in example.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # we will use that template to create a prompt_template\n",
    "# example_pt = PromptTemplate(\n",
    "#     template=example_template,\n",
    "#     input_variables=[\"question\", \"answer\"],\n",
    "# )\n",
    "\n",
    "# # Checking the format of our examples prompt template.\n",
    "# example_pt.invoke(user_provided_examples[0]).to_string()\n",
    "\n",
    "\n",
    "# system_message = ''' A Human is trying to teach you something new by giving examples below in the form of question and answers. For every question there is an answer provided that is suitable for the human. Try to learn from these examples and answer the final question asked by the user. Examples are - '''\n",
    "\n",
    "\n",
    "\n",
    "# # Then we'll create a few shots prompt template using our examples and example_prompt_template\n",
    "# fewshot_pt = FewShotPromptTemplate(\n",
    "#     examples = user_provided_examples,\n",
    "#     example_prompt = example_pt,\n",
    "\n",
    "#     prefix = system_message,                # We can also pass a system message to using prefix.\n",
    "#     suffix=\"Question: {question}\",          # suffix is a prompt_template for final question, which is not an example.\n",
    "#     input_variables=[\"question\"],\n",
    "# )\n",
    "\n",
    "\n",
    "# # Finally we'll invoke the fewshot_pt to create final prompt that will be sent to the llm.\n",
    "# final_prompt = fewshot_pt.invoke({\"question\": \"what is 10 c 2 ?\"})\n",
    "# print(final_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koala-1g7p3nOd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
