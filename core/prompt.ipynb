{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📗 Prompt template Basics 📗 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3\",temperature=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📄 Prompt templates using from_template method 📄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tell me a funny joke about elephant\n",
      "text='tell me a funny joke about elephant'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 1st way of creating a prompt template. \n",
    "# using \"from_template\" method of PromptTemplate class.\n",
    "pt = PromptTemplate.from_template('tell me a funny joke about {content}')     # --> returns an object of PromptTemplate class.\n",
    "\n",
    "\n",
    "# we can format it just like string formatting.\n",
    "print(pt.format(content = 'elephant'))                  # --> prints prompt as string.\n",
    "\n",
    "\n",
    "# we can also invoke a pt object as it is runnable.\n",
    "final_prompt = pt.invoke({\"content\":\"elephant\"})        \n",
    "final_prompt                                            # --> returns StringPromptValue instance\n",
    "print(final_prompt)                                     # --> prints a string variable with prompt as its value.                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🛠️ prompt templates using constructors 🛠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a good joke about elephant\n",
      "text='Tell me a funny joke about elephant'\n"
     ]
    }
   ],
   "source": [
    "# using PromptTemplate class, using PromptTemplate class constructor directly.\n",
    "\n",
    "pt = PromptTemplate(\n",
    "    template='Tell me a {adjective} joke about {content}',\n",
    "    input_variables=['adjective', 'content']\n",
    ")\n",
    "\n",
    "print(pt.format(adjective='good',content= 'elephant'))             # --> prints prompt as string.\n",
    "\n",
    "final_prompt = pt.invoke({\"adjective\" : \"funny\", \"content\" : \"elephant\"})\n",
    "final_prompt                                                        # --> returns StringPromptValue instance\n",
    "print(final_prompt)                                                 # --> prints a string variable with prompt as its value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🦙 Send prompt to LLM 🦙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the elephant quit the circus?\\n\\nBecause it was tired of working for peanuts and wanted to trunk its own business!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# > we can send the prompt to llm by two ways. <#\n",
    "# >   1. Directly invoking the llm\n",
    "# >   2. Creating a chain of two objects.\n",
    "\n",
    "\n",
    "# 1st is directly invoking the llm object.\n",
    "# llm.invoke(final_prompt)                            # --> returns a non parsed string output.\n",
    "\n",
    "\n",
    "# 2nd way we can chain prompt and llm object. chain direction is left --> right.\n",
    "chain = pt | llm\n",
    "chain.invoke(\n",
    "    {\"adjective\": \"new\", \"content\": \"elephant\"}\n",
    ")  # --> returns a non parsed string output.\n",
    "\n",
    "# Second way with chaining is preferred !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📖 few shot example prompt templates 📖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=' A Human is trying to teach you something new by giving examples below in the form of question and answers. For every question there is an answer provided that is suitable for the human. Try to learn from these examples and answer the final question asked by the user. Examples are - \\n\\n\\n    Question : what is 2 c 2 ?\\n    Answer : 4\\n\\n\\n\\n    Question : what is 5 c 3 ?\\n    Answer : 15\\n\\n\\n\\n    Question : what is 12 c 2 ?\\n    Answer : 24\\n\\n\\nQuestion: what is 10 c 2 ?'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "# first create a list of dictionaries as examples.\n",
    "user_provided_examples: List[Dict[str, str]] = [\n",
    "    {\n",
    "        \"question\": \"what is 2 c 2 ?\",\n",
    "        \"answer\": \"4\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is 5 c 3 ?\",\n",
    "        \"answer\": \"15\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is 12 c 2 ?\",\n",
    "        \"answer\": \"24\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# then we will create a template showing how to present examples to the LLM.\n",
    "example_template = \"\"\"\n",
    "    Question : {question}\n",
    "    Answer : {answer}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# we will use that template to create a prompt_template\n",
    "example_pt = PromptTemplate(\n",
    "    template=example_template,\n",
    "    input_variables=[\"question\", \"answer\"],\n",
    ")\n",
    "\n",
    "# Checking the format of our examples prompt template.\n",
    "example_pt.invoke(user_provided_examples[0]).to_string()\n",
    "\n",
    "\n",
    "system_message = ''' A Human is trying to teach you something new by giving examples below in the form of question and answers. For every question there is an answer provided that is suitable for the human. Try to learn from these examples and answer the final question asked by the user. Examples are - '''\n",
    "\n",
    "\n",
    "\n",
    "# Then we'll create a few shots prompt template using our examples and example_prompt_template\n",
    "fewshot_pt = FewShotPromptTemplate(\n",
    "    examples = user_provided_examples,\n",
    "    example_prompt = example_pt,\n",
    "\n",
    "    prefix = system_message,                # We can also pass a system message to using prefix.\n",
    "    suffix=\"Question: {question}\",          # suffix is a prompt_template for final question, which is not an example.\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Finally we'll invoke the fewshot_pt to create final prompt that will be sent to the llm.\n",
    "final_prompt = fewshot_pt.invoke({\"question\": \"what is 10 c 2 ?\"})\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⛓️‍💥 Chaining all the parts together ⛓️‍💥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# use String parser to get output in string \n",
    "str_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "chain = fewshot_pt | llm | str_parser\n",
    "# chain.invoke({\"question\": \"In the above example please explain what does m stands for and what is 12 m 12 ?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🦙 Stream the FewShotExample prompt result from LLM 🦙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think I've learned something new!\n",
      "\n",
      "Based on the examples, it seems that \"c\" represents multiplication.\n",
      " Therefore, if we apply this to the final question:\n",
      "\n",
      "What is 12 c 12?\n",
      "\n",
      "Answer: 144\n"
     ]
    }
   ],
   "source": [
    "line = ''\n",
    "\n",
    "# If we want to stream output line by line.\n",
    "for chunk in chain.stream({\"question\": \"what is 12 c 12 ?\"}):\n",
    "    if chunk == '.' or chunk == '\\n':\n",
    "        line += chunk\n",
    "        print(line)\n",
    "        line = \"\"\n",
    "    \n",
    "    else:\n",
    "        line += chunk\n",
    "\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📖 FewShotExamples with example selection 📖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_chroma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexample_selectors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SemanticSimilarityExampleSelector\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# first create an extensive list of dictionaries as examples.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m user_provided_examples: List[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     {\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is 2 c 2 ?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     },\n\u001b[1;32m     33\u001b[0m ]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_openai'"
     ]
    }
   ],
   "source": [
    "\n",
    "#? from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# first create an extensive list of dictionaries as examples.\n",
    "user_provided_examples: List[Dict[str, str]] = [\n",
    "    {\n",
    "        \"question\": \"what is 2 c 2 ?\",\n",
    "        \"answer\": \"4\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is 5 c 3 ?\",\n",
    "        \"answer\": \"15\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is 12 c 2 ?\",\n",
    "        \"answer\": \"24\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is 5 m 2 ?\",\n",
    "        \"answer\": \"2.5\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is 12 m 3 ?\",\n",
    "        \"answer\": \"4\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is 15 m 15 ?\",\n",
    "        \"answer\": \"1\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# Create an example selector \n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    user_provided_examples,                 # This is the list of examples available to select from.\n",
    "    OpenAIEmbeddings(),                     # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    Chroma,                                 # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    k=2,                                    # This is the number of examples to produce.\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Select the most similar example to the input.\n",
    "question = \"what is 10 c 2 ?\"\n",
    "selected_examples = example_selector.select_examples({\"question\": question})\n",
    "print(f\"Examples most similar to the input: {question}\")\n",
    "for example in selected_examples:\n",
    "    print(\"\\n\")\n",
    "    for k, v in example.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "\n",
    "# # then we will create a template showing how to present examples to the LLM.\n",
    "# example_template = \"\"\"\n",
    "#     Question : {question}\n",
    "#     Answer : {answer}\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# # we will use that template to create a prompt_template\n",
    "# example_pt = PromptTemplate(\n",
    "#     template=example_template,\n",
    "#     input_variables=[\"question\", \"answer\"],\n",
    "# )\n",
    "\n",
    "# # Checking the format of our examples prompt template.\n",
    "# example_pt.invoke(user_provided_examples[0]).to_string()\n",
    "\n",
    "\n",
    "# system_message = ''' A Human is trying to teach you something new by giving examples below in the form of question and answers. For every question there is an answer provided that is suitable for the human. Try to learn from these examples and answer the final question asked by the user. Examples are - '''\n",
    "\n",
    "\n",
    "\n",
    "# # Then we'll create a few shots prompt template using our examples and example_prompt_template\n",
    "# fewshot_pt = FewShotPromptTemplate(\n",
    "#     examples = user_provided_examples,\n",
    "#     example_prompt = example_pt,\n",
    "\n",
    "#     prefix = system_message,                # We can also pass a system message to using prefix.\n",
    "#     suffix=\"Question: {question}\",          # suffix is a prompt_template for final question, which is not an example.\n",
    "#     input_variables=[\"question\"],\n",
    "# )\n",
    "\n",
    "\n",
    "# # Finally we'll invoke the fewshot_pt to create final prompt that will be sent to the llm.\n",
    "# final_prompt = fewshot_pt.invoke({\"question\": \"what is 10 c 2 ?\"})\n",
    "# print(final_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koala-1g7p3nOd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
